{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *       # for datatype conversion\n",
    "from pyspark.sql.functions import *   # for col() function\n",
    "import pandas as pd\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Hotel_Reviews\") \\\n",
    "    .config(\"spark.executor.memory\", '2g') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '1') \\\n",
    "    .config(\"spark.driver.memory\",'1g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data=os.path.join('data/Hotel_Reviews.csv')\n",
    "df=spark.read.csv(path_to_data,header=True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Hotel_Address: string (nullable = true)\n",
      " |-- Additional_Number_of_Scoring: integer (nullable = true)\n",
      " |-- Review_Date: string (nullable = true)\n",
      " |-- Average_Score: double (nullable = true)\n",
      " |-- Hotel_Name: string (nullable = true)\n",
      " |-- Reviewer_Nationality: string (nullable = true)\n",
      " |-- Negative_Review: string (nullable = true)\n",
      " |-- Review_Total_Negative_Word_Counts: integer (nullable = true)\n",
      " |-- Total_Number_of_Reviews: integer (nullable = true)\n",
      " |-- Positive_Review: string (nullable = true)\n",
      " |-- Review_Total_Positive_Word_Counts: integer (nullable = true)\n",
      " |-- Total_Number_of_Reviews_Reviewer_Has_Given: integer (nullable = true)\n",
      " |-- Reviewer_Score: double (nullable = true)\n",
      " |-- Tags: string (nullable = true)\n",
      " |-- days_since_review: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- lng: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515738"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"Positive_Review\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515738"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"Negative_Review\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,col\n",
    "reviews_pos=df.select(['Positive_Review']).withColumnRenamed(\"Positive_Review\", \"Review\")\n",
    "reviews_neg=df.select(['Negative_Review']).withColumnRenamed(\"Negative_Review\", \"Review\")\n",
    "reviews_pos = reviews_pos.withColumn(\"label\",lit(1))\n",
    "reviews_neg = reviews_neg.withColumn(\"label\",lit(0))\n",
    "reviews_final = reviews_pos.union(reviews_neg)\n",
    "reviews_final.dropna()\n",
    "#reviews_final.reset_index(drop=True,inplace=True)\n",
    "reviews_final_temp = reviews_final.filter(col(\"Review\") != 'No Positive').filter(col(\"Review\") != 'No Negative')\n",
    "#my_df.dropna(inplace=True)\n",
    "#my_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = reviews_final_temp.randomSplit([0.80, 0.1, 0.1], seed = 314)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|              Review|label|               words|                  tf|            features|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "|                    |    1|                  []|       (65536,[],[])|       (65536,[],[])|\n",
      "| 1 A very good lo...|    1|[, 1, a, very, go...|(65536,[3921,3977...|(65536,[3921,3977...|\n",
      "| 1 All staff they...|    1|[, 1, all, staff,...|(65536,[4488,1181...|(65536,[4488,1181...|\n",
      "| 1 Amazing staff ...|    1|[, 1, amazing, st...|(65536,[7805,1544...|(65536,[7805,1544...|\n",
      "| 1 Ambient light ...|    1|[, 1, ambient, li...|(65536,[418,493,6...|(65536,[418,493,6...|\n",
      "| 1 Bar at the top...|    1|[, 1, bar, at, th...|(65536,[3067,6393...|(65536,[3067,6393...|\n",
      "| 1 Bathroom is la...|    1|[, 1, bathroom, i...|(65536,[5381,6293...|(65536,[5381,6293...|\n",
      "| 1 Bed was very c...|    1|[, 1, bed, was, v...|(65536,[13432,154...|(65536,[13432,154...|\n",
      "| 1 Breakfast and ...|    1|[, 1, breakfast, ...|(65536,[14,731,15...|(65536,[14,731,15...|\n",
      "| 1 Breakfast was ...|    1|[, 1, breakfast, ...|(65536,[3067,1343...|(65536,[3067,1343...|\n",
      "| 1 Central locati...|    1|[, 1, central, lo...|(65536,[5068,6290...|(65536,[5068,6290...|\n",
      "| 1 Central locati...|    1|[, 1, central, lo...|(65536,[374,601,1...|(65536,[374,601,1...|\n",
      "| 1 Close to touri...|    1|[, 1, close, to, ...|(65536,[1778,4488...|(65536,[1778,4488...|\n",
      "| 1 Comfortable be...|    1|[, 1, comfortable...|(65536,[15445,221...|(65536,[15445,221...|\n",
      "| 1 Croissants wer...|    1|[, 1, croissants,...|(65536,[14,697,31...|(65536,[14,697,31...|\n",
      "| 1 Decent room co...|    1|[, 1, decent, roo...|(65536,[1836,7805...|(65536,[1836,7805...|\n",
      "| 1 Distance to Ei...|    1|[, 1, distance, t...|(65536,[8436,8903...|(65536,[8436,8903...|\n",
      "| 1 Each room is d...|    1|[, 1, each, room,...|(65536,[3198,6309...|(65536,[3198,6309...|\n",
      "| 1 Excellent Loca...|    1|[, 1, excellent, ...|(65536,[4405,7284...|(65536,[4405,7284...|\n",
      "| 1 Excellent loca...|    1|[, 1, excellent, ...|(65536,[7805,1544...|(65536,[7805,1544...|\n",
      "| 1 Excellent loca...|    1|[, 1, excellent, ...|(65536,[7284,7805...|(65536,[7284,7805...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###hashing TF, IDF, then logistic regression\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Review\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "#label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9748290469503991"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9331365974646335"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9401638040712468\n",
      "ROC-AUC:  0.9782662298284932\n"
     ]
    }
   ],
   "source": [
    "###CountVectorizer, IDF, and then Logistic Regression\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Review\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print (\"Accuracy Score: \", format(accuracy))\n",
    "print (\"ROC-AUC: \", format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### N-gram\n",
    "###### extract around 16,000 features from unigram, bigram, trigram. \n",
    "###### This means around 48,000 features in total were extracted. \n",
    "###### Then implementing Chi-Squared feature selection to reduce the number of features to 16,000 in total.\n",
    "\n",
    "from pyspark.ml.feature import NGram, VectorAssembler\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "def ngrams_function(inputCol=[\"Review\",\"target\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"Review\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9522503180661578\n",
      "ROC-AUC:  0.9859813965327662\n"
     ]
    }
   ],
   "source": [
    "trigram_pipelineFit = ngrams_function().fit(train_set)\n",
    "predictions = trigram_pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "# print accuracy, roc_auc\n",
    "print (\"Accuracy Score: \", format(accuracy))\n",
    "print (\"ROC-AUC: \", format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9522503180661578\n",
      "ROC-AUC:  0.9859813965327662\n"
     ]
    }
   ],
   "source": [
    "###### fit the model on test set\n",
    "\n",
    "test_predictions = trigram_pipelineFit.transform(test_set)\n",
    "test_accuracy = test_predictions.filter(test_predictions.label == test_predictions.prediction).count() / float(test_set.count())\n",
    "test_roc_auc = evaluator.evaluate(test_predictions)\n",
    "print (\"Accuracy Score: \", format(accuracy))\n",
    "print (\"ROC-AUC: \", format(roc_auc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------+-----------+\n",
      "|Reviewer_Nationality| Hotel_Name|Reviewer_Score|Review_Date|\n",
      "+--------------------+-----------+--------------+-----------+\n",
      "|             Russia |Hotel Arena|           2.9|   8/3/2017|\n",
      "|            Ireland |Hotel Arena|           7.5|   8/3/2017|\n",
      "|          Australia |Hotel Arena|           7.1|  7/31/2017|\n",
      "|     United Kingdom |Hotel Arena|           3.8|  7/31/2017|\n",
      "|        New Zealand |Hotel Arena|           6.7|  7/24/2017|\n",
      "+--------------------+-----------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### build recommendation engine for hotel ratings ,\"Hotel_Name\",\"Reviewer_Score\",\"Review_Date\"\n",
    "recomm_input = df.select([\"Reviewer_Nationality\",\"Hotel_Name\",\"Reviewer_Score\",\"Review_Date\"])\n",
    "recomm_input.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = recomm_input.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### build recommendation engine for hotel ratings \n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=long(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                Tags|Tags_Index|\n",
      "+--------------------+----------+\n",
      "|[' Leisure trip '...|   19599.0|\n",
      "|[' Leisure trip '...|    3860.0|\n",
      "|[' Leisure trip '...|   12270.0|\n",
      "|[' Leisure trip '...|   18300.0|\n",
      "|[' Leisure trip '...|     566.0|\n",
      "|[' Leisure trip '...|   29918.0|\n",
      "|[' Leisure trip '...|   19836.0|\n",
      "|[' Leisure trip '...|    2297.0|\n",
      "|[' Leisure trip '...|    4186.0|\n",
      "|[' Leisure trip '...|   33867.0|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tags = df.select([\"Tags\"])\n",
    "indexer_tags = StringIndexer(inputCol=\"Tags\", outputCol=\"Tags_Index\")\n",
    "indexed_tags = indexer_tags.fit(df_tags).transform(df_tags)\n",
    "indexed_tags.show(10)\n",
    "#df_tags.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| Hotel_Name|hotel_Index|\n",
      "+-----------+-----------+\n",
      "|Hotel Arena|      395.0|\n",
      "|Hotel Arena|      395.0|\n",
      "|Hotel Arena|      395.0|\n",
      "|Hotel Arena|      395.0|\n",
      "|Hotel Arena|      395.0|\n",
      "|Hotel Arena|      395.0|\n",
      "|Hotel Arena|      395.0|\n",
      "|Hotel Arena|      395.0|\n",
      "|Hotel Arena|      395.0|\n",
      "|Hotel Arena|      395.0|\n",
      "+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hotelname = df.select([\"Hotel_Name\"])\n",
    "indexer_hotel = StringIndexer(inputCol=\"Hotel_Name\", outputCol=\"hotel_Index\")\n",
    "indexed_hotel = indexer_hotel.fit(df_hotelname).transform(df_hotelname)\n",
    "indexed_hotel.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|Reviewer_Nationality|nationality_Index|\n",
      "+--------------------+-----------------+\n",
      "|             Russia |             18.0|\n",
      "|            Ireland |              3.0|\n",
      "|          Australia |              2.0|\n",
      "|     United Kingdom |              0.0|\n",
      "|        New Zealand |             24.0|\n",
      "|             Poland |             27.0|\n",
      "|     United Kingdom |              0.0|\n",
      "|     United Kingdom |              0.0|\n",
      "|            Belgium |             13.0|\n",
      "|             Norway |             31.0|\n",
      "+--------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nationality = df.select([\"Reviewer_Nationality\"])\n",
    "indexer_nationality = StringIndexer(inputCol=\"Reviewer_Nationality\", outputCol=\"nationality_Index\")\n",
    "indexed_nationality = indexer_nationality.fit(df_nationality).transform(df_nationality)\n",
    "indexed_nationality.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Reviewer_Score|\n",
      "+--------------+\n",
      "|           2.9|\n",
      "|           7.5|\n",
      "|           7.1|\n",
      "|           3.8|\n",
      "|           6.7|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_score = df.select([\"Reviewer_Score\"])\n",
    "df_score.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|          features|hotel_Index|\n",
      "+------------------+-----------+\n",
      "|[18.0,19599.0,2.9]|      395.0|\n",
      "|  [3.0,3860.0,7.5]|      395.0|\n",
      "| [2.0,12270.0,7.1]|      395.0|\n",
      "| [0.0,18300.0,3.8]|      395.0|\n",
      "|  [24.0,566.0,6.7]|      395.0|\n",
      "+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df_recomm = df.select(\"Reviewer_Score\",\"Reviewer_Nationality\",\"Tags\",\"Hotel_Name\")\n",
    "\n",
    "indexer_nationality = StringIndexer(inputCol=\"Reviewer_Nationality\", outputCol=\"nationality_Index\")\n",
    "indexed_nationality = indexer_nationality.fit(df_recomm).transform(df_recomm)\n",
    "\n",
    "indexer_hotel = StringIndexer(inputCol=\"Hotel_Name\", outputCol=\"hotel_Index\")\n",
    "indexed_hotel = indexer_hotel.fit(indexed_nationality).transform(indexed_nationality)\n",
    "\n",
    "indexer_tags = StringIndexer(inputCol=\"Tags\", outputCol=\"Tags_Index\")\n",
    "indexed_tags = indexer_tags.fit(indexed_hotel).transform(indexed_hotel)\n",
    "\n",
    "df_recommendation = indexed_tags.drop(\"Reviewer_Nationality\",\"Tags\",\"Hotel_Name\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"nationality_Index\",\"Tags_Index\",\"Reviewer_Score\"],outputCol=\"features\")\n",
    "df_recommendation_final = assembler.transform(df_recommendation).select(\"features\",\"hotel_Index\")\n",
    "df_recommendation_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans(featuresCol=\"features\",k=6, seed=314)  # 6 clusters here\n",
    "#model = kmeans.fit(df_recommendation_final.select(\"features\"))\n",
    "model = kmeans.fit(df_recommendation_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+----------+\n",
      "|          features|hotel_Index|prediction|\n",
      "+------------------+-----------+----------+\n",
      "|[18.0,19599.0,2.9]|      395.0|         2|\n",
      "|  [3.0,3860.0,7.5]|      395.0|         3|\n",
      "| [2.0,12270.0,7.1]|      395.0|         0|\n",
      "| [0.0,18300.0,3.8]|      395.0|         0|\n",
      "|  [24.0,566.0,6.7]|      395.0|         1|\n",
      "+------------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_kmeans = model.transform(df_recommendation_final)\n",
    "transformed_kmeans.show(5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cost = np.zeros(50)\n",
    "for k in range(2,50):\n",
    "    kmeans = KMeans()\\\n",
    "            .setK(k)\\\n",
    "            .setSeed(314) \\\n",
    "            .setFeaturesCol(\"features\")\\\n",
    "            .setPredictionCol(\"cluster\")\n",
    "\n",
    "    model = kmeans.fit(df_recommendation_final)\n",
    "    cost[k] = model.computeCost(df_recommendation_final) # requires Spark 2.0 or later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbs\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,50),cost[2:50])\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_ylabel(\"cost\")\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "kmeans = KMeans() \\\n",
    "          .setK(6) \\\n",
    "          .setFeaturesCol(\"features\")\\\n",
    "          .setPredictionCol(\"cluster\")\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\",\\\n",
    "                               maxCategories=3).fit(df_recommendation_final)\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, kmeans])\n",
    "\n",
    "model = pipeline.fit(df_recommendation_final)\n",
    "\n",
    "cluster = model.transform(df_recommendation_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+------------------+-------+\n",
      "|          features|hotel_Index|   indexedFeatures|cluster|\n",
      "+------------------+-----------+------------------+-------+\n",
      "|[18.0,19599.0,2.9]|      395.0|[18.0,19599.0,2.9]|      0|\n",
      "|  [3.0,3860.0,7.5]|      395.0|  [3.0,3860.0,7.5]|      4|\n",
      "| [2.0,12270.0,7.1]|      395.0| [2.0,12270.0,7.1]|      2|\n",
      "| [0.0,18300.0,3.8]|      395.0| [0.0,18300.0,3.8]|      0|\n",
      "|  [24.0,566.0,6.7]|      395.0|  [24.0,566.0,6.7]|      1|\n",
      "|[27.0,29918.0,6.7]|      395.0|[27.0,29918.0,6.7]|      5|\n",
      "| [0.0,19836.0,4.6]|      395.0| [0.0,19836.0,4.6]|      0|\n",
      "| [0.0,2297.0,10.0]|      395.0| [0.0,2297.0,10.0]|      1|\n",
      "| [13.0,4186.0,6.5]|      395.0| [13.0,4186.0,6.5]|      4|\n",
      "|[31.0,33867.0,7.9]|      395.0|[31.0,33867.0,7.9]|      5|\n",
      "+------------------+-----------+------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------+\n",
      "|          Hotel_name|Users|Total_score|\n",
      "+--------------------+-----+-----------+\n",
      "|Britannia Interna...| 4789|    32692.8|\n",
      "| Strand Palace Hotel| 4256|    34596.4|\n",
      "|Park Plaza Westmi...| 4169|    36073.8|\n",
      "|Copthorne Tara Ho...| 3578|    28961.2|\n",
      "|DoubleTree by Hil...| 3212|    27824.8|\n",
      "+--------------------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the most top n hotels\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "pop = df.groupBy('Hotel_name')\\\n",
    "  .agg(F.count('Reviewer_Score').alias('Users'),F.round(F.sum('Reviewer_Score'),2).alias('Total_score'))\\\n",
    "  .sort([F.col('Users'),F.col('Total_score')],ascending=[0,0])\n",
    "\n",
    "pop.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tags_Index',\n",
       " 'Britannia International Hotel Canary Wharf',\n",
       " 'Strand Palace Hotel',\n",
       " 'Park Plaza Westminster Bridge London',\n",
       " 'Copthorne Tara Hotel London Kensington',\n",
       " 'DoubleTree by Hilton Hotel London Tower of London',\n",
       " 'Grand Royale London Hyde Park',\n",
       " 'Holiday Inn London Kensington',\n",
       " 'Hilton London Metropole',\n",
       " 'Millennium Gloucester Hotel London',\n",
       " 'Intercontinental London The O2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fetch the top 10 hotels\n",
    "top = 10\n",
    "Hotel_name_lst = pd.DataFrame(pop.select('Hotel_name').head(top)).astype('str').iloc[:, 0].tolist()\n",
    "Hotel_name_lst.insert(0,'Tags_Index')\n",
    "Hotel_name_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+\n",
      "| Hotel_name|Reviewer_Score|Tags_Index|\n",
      "+-----------+--------------+----------+\n",
      "|Hotel Arena|           2.9|   19599.0|\n",
      "|Hotel Arena|           7.5|    3860.0|\n",
      "|Hotel Arena|           7.1|   12270.0|\n",
      "|Hotel Arena|           3.8|   18300.0|\n",
      "|Hotel Arena|           6.7|     566.0|\n",
      "+-----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_als = df.select(\"Hotel_name\",\"Reviewer_Score\",\"Tags\")\n",
    "indexer_tags = StringIndexer(inputCol=\"Tags\", outputCol=\"Tags_Index\")\n",
    "df_adjust = indexer_tags.fit(df_als).transform(df_als).drop(\"Tags\")\n",
    "df_adjust.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create portfolio table for each tags index\n",
    "pivot_tab = df_adjust.groupBy('Tags_Index').pivot('Hotel_name').sum('Reviewer_Score')\n",
    "#pivot_tab.show(5)\n",
    "#.pivot('Hotel_name').sum('Reviewer_Score')\n",
    "pivot_tab = pivot_tab.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------------+-------------------+------------------------------------+--------------------------------------+-------------------------------------------------+-----------------------------+-----------------------------+-----------------------+----------------------------------+------------------------------+\n",
      "|Tags_Index|Britannia International Hotel Canary Wharf|Strand Palace Hotel|Park Plaza Westminster Bridge London|Copthorne Tara Hotel London Kensington|DoubleTree by Hilton Hotel London Tower of London|Grand Royale London Hyde Park|Holiday Inn London Kensington|Hilton London Metropole|Millennium Gloucester Hotel London|Intercontinental London The O2|\n",
      "+----------+------------------------------------------+-------------------+------------------------------------+--------------------------------------+-------------------------------------------------+-----------------------------+-----------------------------+-----------------------+----------------------------------+------------------------------+\n",
      "|     769.0|                                      30.5|                0.0|                                 0.0|                                   0.0|                                              0.0|                          0.0|                         50.1|                    0.0|                               0.0|                           0.0|\n",
      "|     558.0|                                       0.0|                0.0|                                 0.0|                                   0.0|                                              0.0|                          0.0|                          0.0|                    0.0|                               0.0|                          29.2|\n",
      "|   28557.0|                                       0.0|                0.0|                                 0.0|                                   0.0|                                              0.0|                          0.0|                          0.0|                    0.0|                               0.0|                           0.0|\n",
      "|   10625.0|                                       0.0|                0.0|                                 0.0|                                   0.0|                                              0.0|                          0.0|                          0.0|                    0.0|                               0.0|                           0.0|\n",
      "|   43010.0|                                       0.0|                0.0|                                 0.0|                                   0.0|                                              0.0|                          0.0|                          0.0|                    0.0|                               0.0|                           0.0|\n",
      "+----------+------------------------------------------+-------------------+------------------------------------+--------------------------------------+-------------------------------------------------+-----------------------------+-----------------------------+-----------------------+----------------------------------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fetch the most n portfolio table for each tags index\n",
    "selected_tab  = pivot_tab.select(Hotel_name_lst)\n",
    "selected_tab.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 124.0 failed 1 times, most recent failure: Lost task 0.0 in stage 124.0 (TID 3003, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-50-bf833a03eaef>\", line 6, in <lambda>\n  File \"<ipython-input-50-bf833a03eaef>\", line 6, in <listcomp>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 44, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-50-bf833a03eaef>\", line 6, in <lambda>\n  File \"<ipython-input-50-bf833a03eaef>\", line 6, in <listcomp>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 44, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-bf833a03eaef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                        for i in range(1,num)]])))\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#temp.take(10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrateings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_tab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \"\"\"\n\u001b[0;32m-> 1378\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 124.0 failed 1 times, most recent failure: Lost task 0.0 in stage 124.0 (TID 3003, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-50-bf833a03eaef>\", line 6, in <lambda>\n  File \"<ipython-input-50-bf833a03eaef>\", line 6, in <listcomp>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 44, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-50-bf833a03eaef>\", line 6, in <lambda>\n  File \"<ipython-input-50-bf833a03eaef>\", line 6, in <listcomp>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 44, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#header = selected_tab.rdd.first()\n",
    "#selected_header = selected_tab.rdd.filter(lambda x: x != header)\n",
    "num = len(selected_tab.columns)\n",
    "temp = selected_tab.rdd.map(lambda x: list(flatten([x[0],[x[i]/float(sum(x[1:]))\n",
    "                                                       if sum(x[1:])>0 else x[i]\n",
    "                                                       for i in range(1,num)]])))\n",
    "#temp.take(10)\n",
    "rateings = spark.createDataFrame(temp,selected_tab.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (3, 4, 5), (3, 6, 7)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1,2,3),(3,4,5),(3,6,7)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 5, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-9-f2abada3170d>\", line 3, in <lambda>\nNameError: name 'num' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-9-f2abada3170d>\", line 3, in <lambda>\nNameError: name 'num' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f2abada3170d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                                        \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                        for i in range(1,num)]])))\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 5, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-9-f2abada3170d>\", line 3, in <lambda>\nNameError: name 'num' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-9-f2abada3170d>\", line 3, in <lambda>\nNameError: name 'num' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "temp = rdd.map(lambda x: list(flatten([x[0],[x[1]/float(sum(x[1:]))\n",
    "                                             #      if sum(x[1:])>0 else x[i]\n",
    "                                               #        for i in range(1,num)]])))\n",
    "temp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 2258, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-29-a480d00bb0fa>\", line 6, in <lambda>\n  File \"<ipython-input-29-a480d00bb0fa>\", line 6, in <listcomp>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 44, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-29-a480d00bb0fa>\", line 6, in <lambda>\n  File \"<ipython-input-29-a480d00bb0fa>\", line 6, in <listcomp>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 44, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a480d00bb0fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melemwiseDiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_tab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-a480d00bb0fa>\u001b[0m in \u001b[0;36melemwiseDiv\u001b[0;34m(df_in)\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                        \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                        for i in range(1,num)]])))\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melemwiseDiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_tab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \"\"\"\n\u001b[0;32m-> 1378\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 2258, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-29-a480d00bb0fa>\", line 6, in <lambda>\n  File \"<ipython-input-29-a480d00bb0fa>\", line 6, in <listcomp>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 44, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-29-a480d00bb0fa>\", line 6, in <lambda>\n  File \"<ipython-input-29-a480d00bb0fa>\", line 6, in <listcomp>\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 44, in _\n    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#build the rating matrix\n",
    "def elemwiseDiv(df_in):\n",
    "    num = len(df_in.columns)\n",
    "    temp = df_in.rdd.map(lambda x: list(flatten([x[0],[x[i]/float(sum(x[1:]))\n",
    "                                                       if sum(x[1:])>0 else x[i]\n",
    "                                                       for i in range(1,num)]])))\n",
    "    return spark.createDataFrame(temp,df_in.columns)\n",
    "\n",
    "ratings = elemwiseDiv(selected_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert rating matrix to long table\n",
    "\n",
    "from pyspark.sql.functions import array, col, explode, struct, lit\n",
    "\n",
    "def to_long(df, by):\n",
    "        \"\"\"\n",
    "        reference: https://stackoverflow.com/questions/37864222/transpose-column-to-row-with-spark\n",
    "        \"\"\"\n",
    "\n",
    "    # Filter dtypes and split into column names and type description\n",
    "    cols, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in by))\n",
    "    # Spark SQL supports only homogeneous columns\n",
    "    assert len(set(dtypes)) == 1, \"All columns have to be of the same type\"\n",
    "\n",
    "    # Create and explode an array of (column_name, column_value) structs\n",
    "    kvs = explode(array([\n",
    "      struct(lit(c).alias(\"Hotel_name\"), col(c).alias(\"rating\")) for c in cols\n",
    "    ])).alias(\"kvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = to_long(ratings,['Tags_Index'])\n",
    "df_all.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the hotel name to numerical index\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "# Index labels, adding metadata to the label column\n",
    "labelIndexer = StringIndexer(inputCol='Hotel_name',\n",
    "                             outputCol='Hotel_index').fit(df_all)\n",
    "df_all = labelIndexer.transform(df_all)\n",
    "\n",
    "df_all.show(5, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_all.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "import sys\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "def computeRmse(model, data):\n",
    "    \"\"\"\n",
    "    Compute RMSE (Root mean Squared Error).\n",
    "    \"\"\"\n",
    "    predictions = model.transform(data)\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(\"Root-mean-square error = \" + str(rmse))\n",
    "    return rmse\n",
    "\n",
    "#train models and evaluate them on the validation set\n",
    "\n",
    "ranks = [4,5]\n",
    "lambdas = [0.05]\n",
    "numIters = [30]\n",
    "bestModel = None\n",
    "bestValidationRmse = float(\"inf\")\n",
    "bestRank = 0\n",
    "bestLambda = -1.0\n",
    "bestNumIter = -1\n",
    "\n",
    "val = test.na.drop()\n",
    "for rank, lmbda, numIter in itertools.product(ranks, lambdas, numIters):\n",
    "    als = ALS(rank=rank, maxIter=numIter, regParam=lmbda, numUserBlocks=10, numItemBlocks=10, implicitPrefs=False,\n",
    "              alpha=1.0,\n",
    "              userCol=\"CustomerID\", itemCol=\"indexedCusip\", seed=1, ratingCol=\"rating\", nonnegative=True)\n",
    "    model=als.fit(train)\n",
    "\n",
    "    validationRmse = computeRmse(model, val)\n",
    "    print(\"RMSE (validation) = %f for the model trained with \" % validationRmse + \\\n",
    "            \"rank = %d, lambda = %.1f, and numIter = %d.\" % (rank, lmbda, numIter))\n",
    "    if (validationRmse, bestValidationRmse):\n",
    "        bestModel = model\n",
    "        bestValidationRmse = validationRmse\n",
    "        bestRank = rank\n",
    "        bestLambda = lmbda\n",
    "        bestNumIter = numIter\n",
    "\n",
    "model = bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction\n",
    "\n",
    "topredict=test[test['rating']==0]\n",
    "\n",
    "predictions=model.transform(topredict)\n",
    "predictions.filter(predictions.prediction>0)\\\n",
    "           .sort([F.col('CustomerID'),F.col('Cusip')],ascending=[0,0]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
