{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *       # for datatype conversion\n",
    "from pyspark.sql.functions import *   # for col() function\n",
    "import pandas as pd\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Hotel_Reviews\") \\\n",
    "    .config(\"spark.executor.memory\", '2g') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '1') \\\n",
    "    .config(\"spark.driver.memory\",'1g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data=os.path.join('data/Hotel_Reviews.csv')\n",
    "df=spark.read.csv(path_to_data,header=True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,col\n",
    "reviews_pos=df.select(['Positive_Review']).withColumnRenamed(\"Positive_Review\", \"Review\")\n",
    "reviews_neg=df.select(['Negative_Review']).withColumnRenamed(\"Negative_Review\", \"Review\")\n",
    "reviews_pos = reviews_pos.withColumn(\"target\",lit(1))\n",
    "reviews_neg = reviews_neg.withColumn(\"target\",lit(0))\n",
    "reviews_final = reviews_pos.union(reviews_neg)\n",
    "#reviews_final.dropna(inplace=True)\n",
    "#reviews_final.reset_index(drop=True,inplace=True)\n",
    "reviews_final_temp = reviews_final.filter(\"Review != 'No Positive'\")\n",
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = reviews_final_temp.randomSplit([0.95, 0.025, 0.025], seed = 314)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|              Review|target|               words|                  tf|            features|label|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "|                    |     1|                  []|       (65536,[],[])|       (65536,[],[])|  1.0|\n",
      "| 1 A very good lo...|     1|[, 1, a, very, go...|(65536,[3921,3977...|(65536,[3921,3977...|  1.0|\n",
      "| 1 All staff they...|     1|[, 1, all, staff,...|(65536,[4488,1181...|(65536,[4488,1181...|  1.0|\n",
      "| 1 Amazing staff ...|     1|[, 1, amazing, st...|(65536,[7805,1544...|(65536,[7805,1544...|  1.0|\n",
      "| 1 Ambient light ...|     1|[, 1, ambient, li...|(65536,[418,493,6...|(65536,[418,493,6...|  1.0|\n",
      "| 1 Apples coffee ...|     1|[, 1, apples, cof...|(65536,[1143,1544...|(65536,[1143,1544...|  1.0|\n",
      "| 1 Bar at the top...|     1|[, 1, bar, at, th...|(65536,[3067,6393...|(65536,[3067,6393...|  1.0|\n",
      "| 1 Bathroom is la...|     1|[, 1, bathroom, i...|(65536,[5381,6293...|(65536,[5381,6293...|  1.0|\n",
      "| 1 Beautiful lobb...|     1|[, 1, beautiful, ...|(65536,[601,767,1...|(65536,[601,767,1...|  1.0|\n",
      "| 1 Bed was very c...|     1|[, 1, bed, was, v...|(65536,[13432,154...|(65536,[13432,154...|  1.0|\n",
      "| 1 Breakfast and ...|     1|[, 1, breakfast, ...|(65536,[14,731,15...|(65536,[14,731,15...|  1.0|\n",
      "| 1 Breakfast was ...|     1|[, 1, breakfast, ...|(65536,[14,697,11...|(65536,[14,697,11...|  1.0|\n",
      "| 1 Breakfast was ...|     1|[, 1, breakfast, ...|(65536,[3067,1343...|(65536,[3067,1343...|  1.0|\n",
      "| 1 Central locati...|     1|[, 1, central, lo...|(65536,[5068,6290...|(65536,[5068,6290...|  1.0|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###hashing TF, IDF, then logistic regression\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Review\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9783991696145778"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9395276717557252"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9401638040712468\n",
      "ROC-AUC:  0.9782662298284932\n"
     ]
    }
   ],
   "source": [
    "###CountVectorizer, IDF, and then Logistic Regression\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Review\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print (\"Accuracy Score: \", format(accuracy))\n",
    "print (\"ROC-AUC: \", format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### N-gram\n",
    "###### extract around 16,000 features from unigram, bigram, trigram. \n",
    "###### This means around 48,000 features in total were extracted. \n",
    "###### Then implementing Chi-Squared feature selection to reduce the number of features to 16,000 in total.\n",
    "\n",
    "from pyspark.ml.feature import NGram, VectorAssembler\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "def ngrams_function(inputCol=[\"Review\",\"target\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"Review\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9522503180661578\n",
      "ROC-AUC:  0.9859813965327662\n"
     ]
    }
   ],
   "source": [
    "trigram_pipelineFit = ngrams_function().fit(train_set)\n",
    "predictions = trigram_pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "# print accuracy, roc_auc\n",
    "print (\"Accuracy Score: \", format(accuracy))\n",
    "print (\"ROC-AUC: \", format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9522503180661578\n",
      "ROC-AUC:  0.9859813965327662\n"
     ]
    }
   ],
   "source": [
    "###### fit the model on test set\n",
    "\n",
    "test_predictions = trigram_pipelineFit.transform(test_set)\n",
    "test_accuracy = test_predictions.filter(test_predictions.label == test_predictions.prediction).count() / float(test_set.count())\n",
    "test_roc_auc = evaluator.evaluate(test_predictions)\n",
    "print (\"Accuracy Score: \", format(accuracy))\n",
    "print (\"ROC-AUC: \", format(roc_auc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------+-----------+\n",
      "|Reviewer_Nationality| Hotel_Name|Reviewer_Score|Review_Date|\n",
      "+--------------------+-----------+--------------+-----------+\n",
      "|             Russia |Hotel Arena|           2.9|   8/3/2017|\n",
      "|            Ireland |Hotel Arena|           7.5|   8/3/2017|\n",
      "|          Australia |Hotel Arena|           7.1|  7/31/2017|\n",
      "|     United Kingdom |Hotel Arena|           3.8|  7/31/2017|\n",
      "|        New Zealand |Hotel Arena|           6.7|  7/24/2017|\n",
      "+--------------------+-----------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### build recommendation engine for hotel ratings ,\"Hotel_Name\",\"Reviewer_Score\",\"Review_Date\"\n",
    "recomm_input = df.select([\"Reviewer_Nationality\",\"Hotel_Name\",\"Reviewer_Score\",\"Review_Date\"])\n",
    "recomm_input.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = recomm_input.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Column Reviewer_Nationality must be of type numeric but was actually of type string.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o426.fit.\n: java.lang.IllegalArgumentException: requirement failed: Column Reviewer_Nationality must be of type numeric but was actually of type string.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:76)\n\tat org.apache.spark.ml.recommendation.ALSParams$class.validateAndTransformSchema(ALS.scala:252)\n\tat org.apache.spark.ml.recommendation.ALS.validateAndTransformSchema(ALS.scala:569)\n\tat org.apache.spark.ml.recommendation.ALS.transformSchema(ALS.scala:691)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$fit$1.apply(ALS.scala:659)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$fit$1.apply(ALS.scala:658)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:658)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4dd14b094950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m als = ALS(maxIter=5, regParam=0.01, userCol=\"Reviewer_Nationality\", itemCol=\"Hotel_Name\", ratingCol=\"Reviewer_Score\",\n\u001b[1;32m      6\u001b[0m           coldStartStrategy=\"drop\")\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Column Reviewer_Nationality must be of type numeric but was actually of type string.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"Reviewer_Nationality\", itemCol=\"Hotel_Name\", ratingCol=\"Reviewer_Score\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### build recommendation engine for hotel ratings \n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=long(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
